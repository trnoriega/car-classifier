{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Brief summary of the experiments in this folder\n",
    "\n",
    "The experiments contained here show the results of experiments trying to modify and fine-tune a pretrained \n",
    "Inceptionv3 network [available in the keras framework](https://keras.io/applications/#inceptionv3)\n",
    "\n",
    "All experiments were run on an amazon p2.xlarge instance with 4 CPUs and 1 NVIDIA GPU. \n",
    "On average they took about 40 min/ten training epochs\n",
    "\n",
    "Ultimately, the model tested in notebooks [Inceptionv3_21](Inceptionv3_21.ipynb) and \n",
    "[Inceptionv3_21_weight_test](Inceptionv3_21_weight_test.ipynb) was used in the implementation \n",
    "in the [Car-Match website](https://github.com/trnoriega/car-website)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial rmsProp experiments:\n",
    "Experiments 1-3, 5-8 focused on finding hyperparameters in the rmsProp optimization algorithm \n",
    "that allowed for the fastest training with the highest accuracy.\n",
    "\n",
    "Main conclusions:\n",
    "\n",
    "- Learning rate is extremely important: even small deviations from 0.001 led to degradation in performance\n",
    "- Without fine tuning deeper layers there was very little that could be done to push \n",
    "__top-3 training accuracy past 0.6598__\n",
    "- Even with optimization of learning rate good performance still needed about 30-50 training epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing other optimization algorithms\n",
    "\n",
    "Experiments 4, 9-12 tested default settings of other optimization algorithms:\n",
    "\n",
    "- SGD; which wasn't very impressive\n",
    "- Adagrad; which after some learning rate fine-tuning outperformed rmsProp in terms of epochs needed to get high accuracy\n",
    "- Adagradelta; which was not as good as Adagrad\n",
    "- Adam; which significantly outperformed rmsProp without need of fine-tuning\n",
    "- Nadam (experiment 12); which took the price for lowest \n",
    "number of epochs to reach a high top-3 accuracy: __0.63 after 20 epochs__\n",
    "\n",
    "__From here on all experiments used the Nadam optimizer with default settings__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine tuning-deeper layers\n",
    "\n",
    "Experiments 13-14 showed that by fine-tunning the next deepest layer after the classification dense-layer\n",
    "there was a large increase in top-3 accuracy: __0.9281 after 30 epochs!__\n",
    "\n",
    "However, this came at the expense of overfitting, as indicated by much worse performance in the validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization\n",
    "\n",
    "Experiments 15-21 tested two different regularization methods:\n",
    "\n",
    "- L2 regularization to the prediction dense layer, which did not work at all. \n",
    "The model never learned at all, regardless of regularization parameters.\n",
    "- Dropout layer between the last convolutional network pooling layer and the prediction dense layer,\n",
    "which worked like a charm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FINAL MODEL\n",
    "\n",
    "[Experiment 21](Inceptionv3_21.ipynb) contains the most succesful model. \n",
    "Progressively finetuning deeper layers over 40 epochs. \n",
    "__Top-5 accuracy on the test set was 95%__. Full validation can be found [here](../5_model_validation.ipynb)\n",
    "\n",
    "Settings:\n",
    "\n",
    "```\n",
    "pred_layer_config = {\n",
    "    'activation': 'softmax',\n",
    "    'activity_regularizer': None,\n",
    "    'bias_constraint': None,\n",
    "    'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
    "    'bias_regularizer': None,\n",
    "    'kernel_constraint': None,\n",
    "    'kernel_initializer': {'class_name': 'VarianceScaling',\n",
    "                           'config': {\n",
    "                               'distribution': 'uniform',\n",
    "                               'mode': 'fan_avg',\n",
    "                               'scale': 1.0,\n",
    "                               'seed': 8}\n",
    "                          },\n",
    "    'kernel_regularizer': None,\n",
    "    'name': 'predictions',\n",
    "    'trainable': True,\n",
    "    'units': NB_CLASSES,\n",
    "    'use_bias': True}\n",
    "\n",
    "model = Sequential()\n",
    "model.add(conv_base)\n",
    "model.add(Dropout(0.5, seed=21))\n",
    "model.add(Dense(**pred_layer_config))\n",
    "\n",
    "optimizer = optimizers.Nadam(lr=0.002, beta_1=0.9, beta_2=0.999, schedule_decay=0.004)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
